{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\raque\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\raque\\.cache\\huggingface\\hub\\models--google--gemma-2-9b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as count_1w.txt\n",
      "Words extracted: 333333\n",
      "Total unique tokens: 37293\n",
      "CSV file with tokenized words saved to: count_1w_tokenized_by_google_gemma-2-9b.csv\n",
      "CSV file with token frequencies saved to: count_1w_frequency_tokens_tokenized_by_google_gemma-2-9b.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "import requests  # To download the file from the URL\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Insert your Hugging Face token here\n",
    "load_dotenv()\n",
    "huggingface_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if huggingface_token:\n",
    "    login(huggingface_token)\n",
    "    print(\"Logged in successfully!\")\n",
    "else:\n",
    "    print(\"Hugging Face token is not set in the .env file.\")\n",
    "\n",
    "# Specify the model name\n",
    "model_name = \"google/gemma-2-9b\"  # Change the model name as needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to remove the special character '▁' from the first token of each word\n",
    "def clean_special_character(tokens):\n",
    "    \"\"\"\n",
    "    Removes the leading special character '▁' from the first token of a word.\n",
    "\n",
    "    Args:\n",
    "        tokens (list of str): List of tokens for a word.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Tokens with the special character removed from the first token.\n",
    "    \"\"\"\n",
    "    if tokens and tokens[0].startswith(\"▁\"):  # Check if the first token starts with '▁'\n",
    "        tokens[0] = tokens[0][1:]  # Remove the first character\n",
    "    return tokens\n",
    "\n",
    "# Function to read the file and extract words and their frequencies\n",
    "def extract_words_and_frequencies(file_path):\n",
    "    words_with_freq = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Each line has the format: word frequency\n",
    "            parts = line.split()\n",
    "            if len(parts) == 2:  # Ensure there's a word and frequency\n",
    "                word = parts[0]\n",
    "                frequency = int(parts[1])\n",
    "                words_with_freq.append((word, frequency))\n",
    "    return words_with_freq\n",
    "\n",
    "# Function to tokenize words and calculate the cumulative frequency of tokens\n",
    "def tokenize_and_count(words_with_freq, tokenizer):\n",
    "    token_counts = Counter()  # To store tokens and their counts\n",
    "    tokenized_words = []  # To store tokens for each word\n",
    "\n",
    "    for word, frequency in words_with_freq:\n",
    "        tokens = tokenizer.tokenize(word)  # Tokenize the word\n",
    "        tokens = clean_special_character(tokens)  # Clean the special character '▁'\n",
    "        num_tokens = len(tokens)\n",
    "        for token in tokens:\n",
    "            token_counts[token] += frequency  # Add the word frequency to the token\n",
    "        tokenized_words.append((word, frequency, num_tokens, tokens))  # Store the word info\n",
    "    \n",
    "    return token_counts, tokenized_words\n",
    "\n",
    "# Function to save words, frequencies, number of tokens, and subwords to a CSV\n",
    "def save_word_tokens_to_csv(tokenized_words, input_file, model_name):\n",
    "    base_name = re.sub(r'\\.txt$', '', input_file.split(\"/\")[-1])  # Remove the .txt extension\n",
    "    output_file_path = f\"{base_name}_tokenized_by_{model_name.replace('/', '_')}.csv\"\n",
    "    \n",
    "    with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        writer.writerow(['Word', 'Frequency', 'Number of Tokens', 'Subwords'])\n",
    "        \n",
    "        # Write each word, frequency, number of tokens, and tokens\n",
    "        for word, frequency, num_tokens, tokens in tokenized_words:\n",
    "            writer.writerow([word, frequency, num_tokens, ' '.join(tokens)])\n",
    "    \n",
    "    print(f\"CSV file with tokenized words saved to: {output_file_path}\")\n",
    "\n",
    "# Function to save tokens and their frequencies to a CSV\n",
    "def save_token_counts_to_csv(token_counts, input_file, model_name):\n",
    "    base_name = re.sub(r'\\.txt$', '', input_file.split(\"/\")[-1])  # Remove the .txt extension\n",
    "    output_file_path = f\"{base_name}_frequency_tokens_tokenized_by_{model_name.replace('/', '_')}.csv\"\n",
    "    \n",
    "    with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        writer.writerow(['Token', 'Frequency'])\n",
    "        \n",
    "        # Write each token and its cumulative frequency\n",
    "        for token, count in token_counts.items():\n",
    "            writer.writerow([token, count])\n",
    "    \n",
    "    print(f\"CSV file with token frequencies saved to: {output_file_path}\")\n",
    "\n",
    "# URL of the file\n",
    "url = \"https://norvig.com/ngrams/count_1w.txt\"\n",
    "local_file_path = \"count_1w.txt\"\n",
    "\n",
    "# Download the file from the URL\n",
    "try:\n",
    "    # Download the file\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(local_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        print(f\"File downloaded and saved as {local_file_path}\")\n",
    "    else:\n",
    "        raise Exception(f\"Error downloading the file. Status code: {response.status_code}\")\n",
    "\n",
    "    # Extract words and their frequencies from the TXT file\n",
    "    words_with_freq = extract_words_and_frequencies(local_file_path)\n",
    "    print(f\"Words extracted: {len(words_with_freq)}\")\n",
    "\n",
    "    # Tokenize words and calculate cumulative token frequencies\n",
    "    token_counts, tokenized_words = tokenize_and_count(words_with_freq, tokenizer)\n",
    "    print(f\"Total unique tokens: {len(token_counts)}\")\n",
    "\n",
    "    # Save tokenized words and frequencies to a CSV\n",
    "    save_word_tokens_to_csv(tokenized_words, local_file_path, model_name)\n",
    "\n",
    "    # Save tokens and their cumulative frequencies to another CSV\n",
    "    save_token_counts_to_csv(token_counts, local_file_path, model_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
