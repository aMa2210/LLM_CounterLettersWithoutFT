This repository contains the data and scripts used to analyze why LLMs struggle to count the letters in words accurately. It explores the limitations of LLMs in handling tasks involving precise numerical computations.

Repository Contents
- Words: Contains the full dataset (count_1w.txt) and a sampled dataset (Random_10000_words.json) used for experiments and analysis.
- Results:
  - Contains the results from 'Words/Random_10000_words.json' generated by various LLMs.
  - JSON files, named after the corresponding LLM models, contain the responses generated by each model with the following prompt: ```'Count the frequency of each letter in the input word and output it in JSON format, generate a JSON format reply directly without any additional information required. Here's an example:
  Input: strawberry
  Output: \{"s": 1, "t": 1, "r": 3, "a": 1, "w": 1, "b": 1, "e": 1, "y": 1\}'.```
  - CSV files contain word and token features. Please check 'Random_10000_words.csv' for most of the data.
  - 'token_results' folder contains the tokenized results of various models.
  - 'Real_Pred_Letters_Tables' folder provides more detailed results at the letter level.
- Figure: Outputs and visualizations derived from the analysis.

- The Python script in the root directory is used for analyzing and plotting the data in the Results folder.
