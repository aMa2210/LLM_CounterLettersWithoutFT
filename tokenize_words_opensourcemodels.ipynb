{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\raque\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Logged in successfully!\n",
      "Processed file saved to: test_words_tokenized_by_meta-llama_Llama-3.1-8B.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Insert your Hugging Face token here\n",
    "load_dotenv()\n",
    "huggingface_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if huggingface_token:\n",
    "    login(huggingface_token)\n",
    "    print(\"Logged in successfully!\")\n",
    "else:\n",
    "    print(\"Hugging Face token is not set in the .env file.\")\n",
    "\n",
    "# Load the tokenizer for Gemma2-9B from Hugging Face\n",
    "model_name = \"meta-llama/Llama-3.1-8B\" #mistralai/Mistral-7B-v0.1, meta-llama/Llama-3.1-8B  , google/gemma-2-9b\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to tokenize a word and get the token count and tokens\n",
    "def tokenize_word(word):\n",
    "    tokens = tokenizer.tokenize(word)  # Tokenize the word\n",
    "    num_tokens = len(tokens)  # Count the number of tokens\n",
    "    return num_tokens, tokens\n",
    "\n",
    "# Read the existing CSV file, process its contents, and add new columns\n",
    "def process_csv(input_csv):\n",
    "    # Generate the output CSV file name dynamically\n",
    "    model_name = tokenizer.name_or_path.replace(\"/\", \"_\")  # Replace slashes in model name to avoid file issues\n",
    "    output_csv = f\"{input_csv.split('.')[0]}_tokenized_by_{model_name}.csv\"\n",
    "\n",
    "    with open(input_csv, 'r', encoding='utf-8') as infile, open(output_csv, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = reader.fieldnames + ['Tokens_Number', 'Tokens']  # Add new columns\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write the header\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Process each row and add the new data\n",
    "        for row in reader:\n",
    "            word = row['Word']\n",
    "            num_tokens, tokens = tokenize_word(word)\n",
    "            row['Tokens_Number'] = num_tokens\n",
    "            row['Tokens'] = ' '.join(tokens)\n",
    "            writer.writerow(row)\n",
    "        print(f\"Processed file saved to: {output_csv}\")\n",
    "\n",
    "# Path to the input CSV file\n",
    "input_csv = 'test_words.csv'  # Original file name\n",
    "\n",
    "# Run the CSV processing function\n",
    "process_csv(input_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
