{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as count_1w.txt\n",
      "Words extracted: 333333\n",
      "Total unique tokens: 22245\n",
      "CSV file with tokenized words saved to: count_1w_tokenized_by_gpt-4o-mini.csv\n",
      "CSV file with token frequencies saved to: count_1w_frequency_tokens_tokenized_by_gpt-4o-mini.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import tiktoken\n",
    "import requests  # To download the file from the URL\n",
    "from collections import Counter\n",
    "\n",
    "# Function to download the file from a URL\n",
    "def download_file(url, local_file_path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(local_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        print(f\"File downloaded and saved as {local_file_path}\")\n",
    "    else:\n",
    "        raise Exception(f\"Error downloading the file. Status code: {response.status_code}\")\n",
    "\n",
    "# Function to read the file and extract words and their frequencies\n",
    "def extract_words_and_frequencies(file_path):\n",
    "    words_with_freq = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Each line has the format: word frequency\n",
    "            parts = line.split()\n",
    "            if len(parts) == 2:  # Ensure there's a word and frequency\n",
    "                word = parts[0]\n",
    "                frequency = int(parts[1])\n",
    "                words_with_freq.append((word, frequency))\n",
    "    return words_with_freq\n",
    "\n",
    "# Function to tokenize words and calculate the cumulative frequency of tokens\n",
    "def tokenize_and_count(words_with_freq, model_name):\n",
    "    enc = tiktoken.encoding_for_model(model_name)  # Use the specified tokenizer\n",
    "    token_counts = Counter()  # To store tokens and their counts\n",
    "    tokenized_words = []  # To store tokens for each word\n",
    "\n",
    "    for word, frequency in words_with_freq:\n",
    "        token_ids = enc.encode(word)  # Tokenize the word\n",
    "        tokens = [enc.decode([token_id]) for token_id in token_ids]\n",
    "        for token in tokens:\n",
    "            token_counts[token] += frequency  # Add the word frequency to the token\n",
    "        tokenized_words.append((word, frequency, len(tokens), tokens))  # Store the word info\n",
    "    \n",
    "    return token_counts, tokenized_words\n",
    "\n",
    "# Function to save words, frequencies, number of tokens, and subwords to a CSV\n",
    "def save_word_tokens_to_csv(tokenized_words, input_file, model_name):\n",
    "    base_name = re.sub(r'\\.txt$', '', input_file.split(\"/\")[-1])  # Remove the .txt extension\n",
    "    output_file_path = f\"{base_name}_tokenized_by_{model_name}.csv\"\n",
    "    \n",
    "    with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        writer.writerow(['Word', 'Frequency', 'Number of Tokens', 'Subwords'])\n",
    "        \n",
    "        # Write each word, frequency, number of tokens, and tokens\n",
    "        for word, frequency, num_tokens, tokens in tokenized_words:\n",
    "            writer.writerow([word, frequency, num_tokens, ' '.join(tokens)])\n",
    "    \n",
    "    print(f\"CSV file with tokenized words saved to: {output_file_path}\")\n",
    "\n",
    "# Function to save tokens and their frequencies to a CSV\n",
    "def save_token_counts_to_csv(token_counts, input_file, model_name):\n",
    "    base_name = re.sub(r'\\.txt$', '', input_file.split(\"/\")[-1])  # Remove the .txt extension\n",
    "    output_file_path = f\"{base_name}_frequency_tokens_tokenized_by_{model_name}.csv\"\n",
    "    \n",
    "    with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        writer.writerow(['Token', 'Frequency'])\n",
    "        \n",
    "        # Write each token and its cumulative frequency\n",
    "        for token, count in token_counts.items():\n",
    "            writer.writerow([token, count])\n",
    "    \n",
    "    print(f\"CSV file with token frequencies saved to: {output_file_path}\")\n",
    "\n",
    "# URL of the file\n",
    "url = \"https://norvig.com/ngrams/count_1w.txt\"\n",
    "local_file_path = \"count_1w.txt\"\n",
    "model_name = \"gpt-4o-mini\"  # Specify the model name\n",
    "\n",
    "# Download the file from the URL\n",
    "try:\n",
    "    # Download the file\n",
    "    download_file(url, local_file_path)\n",
    "\n",
    "    # Extract words and their frequencies from the TXT file\n",
    "    words_with_freq = extract_words_and_frequencies(local_file_path)\n",
    "    print(f\"Words extracted: {len(words_with_freq)}\")\n",
    "\n",
    "    # Tokenize words and calculate cumulative token frequencies\n",
    "    token_counts, tokenized_words = tokenize_and_count(words_with_freq, model_name)\n",
    "    print(f\"Total unique tokens: {len(token_counts)}\")\n",
    "\n",
    "    # Save tokenized words and frequencies to a CSV\n",
    "    save_word_tokens_to_csv(tokenized_words, local_file_path, model_name)\n",
    "\n",
    "    # Save tokens and their cumulative frequencies to another CSV\n",
    "    save_token_counts_to_csv(token_counts, local_file_path, model_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
